import asyncio
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
import structlog
from sqlalchemy.orm import Session
from sqlalchemy.sql import func

from models.user import User
from models.preference import Preference
from models.repo_cache import RepoCache
from models.job_run import JobRun
from models.recommendation import Recommendation
from services.github_client import GitHubClient, GitHubRateLimitError
from services.recommendation_engine import RecommendationEngine
from services.notification_service import NotificationService

logger = structlog.get_logger()


class JobExecutionService:
    def __init__(self, db: Session):
        self.db = db
        self.recommendation_engine = RecommendationEngine(db)
        self.notification_service = NotificationService(db)

    async def execute_recommendation_job(
        self,
        user_id: int,
        job_run_id: int,
        preference_id: Optional[int] = None,
        force_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        Execute a recommendation job for a user
        """
        job_run = self.db.query(JobRun).filter(JobRun.id == job_run_id).first()
        if not job_run:
            raise ValueError(f"Job run {job_run_id} not found")

        try:
            # Update job status
            job_run.status = "running"
            job_run.started_at = func.now()
            self.db.commit()

            logger.info("Starting recommendation job", user_id=user_id, job_run_id=job_run_id)

            # æ‰“å°å¼€å§‹ä¿¡æ¯
            print(f"\nðŸš€ å¼€å§‹æ‰§è¡ŒæŽ¨èä»»åŠ¡")
            print(f"   - ç”¨æˆ·ID: {user_id}")
            print(f"   - ä»»åŠ¡ID: {job_run_id}")
            print(f"   - æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            if preference_id:
                print(f"   - æŒ‡å®šé…ç½®ID: {preference_id}")
            print(f"   - å¼ºåˆ¶åˆ·æ–°: {'æ˜¯' if force_refresh else 'å¦'}")

            # Get user and preferences
            user = self.db.query(User).filter(User.id == user_id).first()
            if not user:
                raise ValueError(f"User {user_id} not found")

            # Get preferences to process
            if preference_id:
                preferences = self.db.query(Preference).filter(
                    Preference.id == preference_id,
                    Preference.user_id == user_id,
                    Preference.enabled == True
                ).all()
            else:
                preferences = self.db.query(Preference).filter(
                    Preference.user_id == user_id,
                    Preference.enabled == True
                ).all()

            if not preferences:
                print(f"\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æŽ¨èé…ç½®")
                job_run.status = "completed"
                job_run.finished_at = func.now()
                job_run.error_message = "No active preferences found"
                self.db.commit()
                return {"status": "completed", "message": "No active preferences found"}

            print(f"\nðŸ“‹ æ‰¾åˆ° {len(preferences)} ä¸ªå¯ç”¨çš„æŽ¨èé…ç½®ï¼Œå‡†å¤‡æŒ‰é…ç½®åˆ†åˆ«å¤„ç†...")

            total_stats = {
                "repos_fetched": 0,
                "repos_cached": 0,
                "repos_filtered": 0,
                "recommendations_generated": 0,
                "preferences_processed": 0,
                "errors_count": 0
            }

            # Process each preference
            async with GitHubClient() as github_client:
                for i, preference in enumerate(preferences, 1):
                    try:
                        # è¯¦ç»†æ‰“å°é…ç½®ä¿¡æ¯
                        print(f"\n{'='*80}")
                        print(f"ðŸ”„ å¤„ç†é…ç½® {i}/{len(preferences)}: {preference.name}")
                        print(f"{'='*80}")
                        print(f"ðŸ“‹ é…ç½®è¯¦æƒ…:")
                        print(f"   - é…ç½®ID: {preference.id}")
                        print(f"   - é…ç½®åç§°: {preference.name}")
                        print(f"   - å…³é”®è¯: {', '.join(preference.keywords) if preference.keywords else 'æ— '}")
                        print(f"   - ç¼–ç¨‹è¯­è¨€: {', '.join(preference.languages) if preference.languages else 'æ— é™åˆ¶'}")
                        print(f"   - æœ€å°Staræ•°: {preference.min_stars}")
                        print(f"   - æœ€å¤§æŽ¨èæ•°: {preference.max_recommendations}")
                        print(f"   - é€šçŸ¥æ¸ é“: {', '.join(preference.notification_channels) if preference.notification_channels else 'æ— '}")
                        print(f"")

                        logger.info("Processing preference", preference_id=preference.id, name=preference.name)

                        # Fetch repositories from GitHub
                        print(f"ðŸ” æ­£åœ¨æœç´¢GitHubé¡¹ç›®...")
                        repos = await self._fetch_repositories(github_client, preference, force_refresh)
                        total_stats["repos_fetched"] += len(repos)
                        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(repos)} ä¸ªç›¸å…³é¡¹ç›®")

                        # Cache repositories
                        print(f"ðŸ’¾ æ­£åœ¨ç¼“å­˜é¡¹ç›®æ•°æ®...")
                        cached_repos = await self._cache_repositories(repos)
                        total_stats["repos_cached"] += len(cached_repos)
                        print(f"âœ… ç¼“å­˜å®Œæˆï¼Œå¤„ç†äº† {len(cached_repos)} ä¸ªé¡¹ç›®")

                        # Filter and score repositories
                        print(f"â­ æ­£åœ¨è¯„åˆ†å’Œè¿‡æ»¤é¡¹ç›®...")
                        filtered_repos = self.recommendation_engine.filter_repositories(
                            [self._repo_cache_to_dict(repo) for repo in cached_repos],
                            preference
                        )
                        total_stats["repos_filtered"] += len(filtered_repos)
                        print(f"âœ… è¿‡æ»¤å®Œæˆï¼Œç­›é€‰å‡º {len(filtered_repos)} ä¸ªé«˜è´¨é‡é¡¹ç›®")

                        # Save recommendations
                        print(f"ðŸ’¾ æ­£åœ¨ä¿å­˜æŽ¨èç»“æžœ...")
                        recommendations = self.recommendation_engine.save_recommendations(
                            user_id=user_id,
                            preference_id=preference.id,
                            job_run_id=job_run_id,
                            filtered_repos=filtered_repos
                        )
                        total_stats["recommendations_generated"] += len(recommendations)
                        print(f"âœ… ä¿å­˜å®Œæˆï¼Œç”Ÿæˆäº† {len(recommendations)} æ¡æŽ¨è")

                        # æ‰“å°æŽ¨èé¡¹ç›®è¯¦æƒ…
                        if recommendations:
                            print(f"\nðŸ“Š æŽ¨èé¡¹ç›®åˆ—è¡¨:")
                            for j, rec in enumerate(recommendations[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                                repo = self.db.query(RepoCache).filter(RepoCache.repo_id == rec.repo_id).first()
                                if repo:
                                    print(f"   {j}. {repo.full_name} (â­{repo.stargazers_count:,} stars, ðŸ’»{repo.language or 'N/A'}, ðŸŽ¯{rec.score:.2f})")
                            if len(recommendations) > 5:
                                print(f"   ... è¿˜æœ‰ {len(recommendations) - 5} ä¸ªæŽ¨èé¡¹ç›®")
                        else:
                            print(f"   æš‚æ— ç¬¦åˆæ¡ä»¶çš„æŽ¨èé¡¹ç›®")

                        # Send notifications if there are new recommendations
                        sent_channels = []
                        if recommendations and preference.notification_channels:
                            try:
                                print(f"\nðŸ“¨ æ­£åœ¨å‘é€é€šçŸ¥åˆ°: {', '.join(preference.notification_channels)}")
                                sent_channels = await self.notification_service.send_recommendations_notification(
                                    user_id=user_id,
                                    preference=preference,
                                    recommendations=recommendations
                                )
                                total_stats[f"notifications_sent_{preference.id}"] = sent_channels

                                if sent_channels:
                                    print(f"âœ… é€šçŸ¥å‘é€æˆåŠŸ: {', '.join(sent_channels)}")
                                else:
                                    print(f"âš ï¸  é€šçŸ¥å‘é€å¤±è´¥æˆ–æ— å¯ç”¨æ¸ é“")

                                logger.info(
                                    "Notifications sent for preference",
                                    preference_id=preference.id,
                                    sent_channels=sent_channels
                                )
                            except Exception as e:
                                print(f"âŒ é€šçŸ¥å‘é€å¤±è´¥: {str(e)}")
                                logger.error(
                                    "Failed to send notifications",
                                    preference_id=preference.id,
                                    error=str(e)
                                )
                                total_stats["notification_errors"] = total_stats.get("notification_errors", 0) + 1
                        elif recommendations:
                            print(f"âš ï¸  è¯¥é…ç½®æœªè®¾ç½®é€šçŸ¥æ¸ é“ï¼Œè·³è¿‡é€šçŸ¥å‘é€")
                        else:
                            print(f"â„¹ï¸  æ— æŽ¨èç»“æžœï¼Œè·³è¿‡é€šçŸ¥å‘é€")

                        total_stats["preferences_processed"] += 1

                        # æ‰“å°é…ç½®å¤„ç†æ‘˜è¦
                        print(f"\nðŸ“Š é…ç½® '{preference.name}' å¤„ç†æ‘˜è¦:")
                        print(f"   - æœç´¢åˆ°çš„é¡¹ç›®: {len(repos)}")
                        print(f"   - ç”Ÿæˆçš„æŽ¨è: {len(recommendations)}")
                        print(f"   - é€šçŸ¥æ¸ é“: {', '.join(sent_channels) if sent_channels else 'æ— '}")
                        print(f"âœ… é…ç½® '{preference.name}' å¤„ç†å®Œæˆ")

                        logger.info(
                            "Preference processed successfully",
                            preference_id=preference.id,
                            preference_name=preference.name,
                            repos_found=len(repos),
                            recommendations_count=len(recommendations),
                            sent_channels=sent_channels
                        )

                    except GitHubRateLimitError as e:
                        logger.warning("Hit GitHub rate limit", reset_time=e.reset_time)
                        total_stats["errors_count"] += 1
                        break  # Stop processing to avoid more rate limit hits

                    except Exception as e:
                        logger.error("Error processing preference", preference_id=preference.id, error=str(e))
                        total_stats["errors_count"] += 1
                        continue

            # Update job completion
            job_run.status = "completed"
            job_run.finished_at = func.now()
            job_run.counters = total_stats
            self.db.commit()

            # æ‰“å°æœ€ç»ˆæ€»ç»“
            print(f"\n{'='*80}")
            print(f"ðŸ“Š æŽ¨èä»»åŠ¡æ‰§è¡Œå®Œæˆ - æ€»ç»“æŠ¥å‘Š")
            print(f"{'='*80}")
            print(f"âœ… å¤„ç†çš„é…ç½®æ•°: {total_stats['preferences_processed']}")
            print(f"ðŸ” æœç´¢åˆ°çš„é¡¹ç›®æ€»æ•°: {total_stats['repos_fetched']}")
            print(f"ðŸ’¾ ç¼“å­˜çš„é¡¹ç›®æ•°: {total_stats['repos_cached']}")
            print(f"â­ è¿‡æ»¤çš„é¡¹ç›®æ•°: {total_stats['repos_filtered']}")
            print(f"ðŸŽ¯ ç”Ÿæˆçš„æŽ¨èæ€»æ•°: {total_stats['recommendations_generated']}")

            # ç»Ÿè®¡é€šçŸ¥å‘é€æƒ…å†µ
            total_notifications = 0
            successful_configs = []
            for key, value in total_stats.items():
                if key.startswith("notifications_sent_") and value:
                    total_notifications += 1
                    config_id = key.split("_")[-1]
                    config_name = next((p.name for p in preferences if str(p.id) == config_id), f"é…ç½®{config_id}")
                    successful_configs.append(f"{config_name}({', '.join(value)})")

            print(f"ðŸ“¨ æˆåŠŸå‘é€é€šçŸ¥çš„é…ç½®: {total_notifications}")
            if successful_configs:
                for config_info in successful_configs:
                    print(f"   - {config_info}")

            if total_stats.get("errors_count", 0) > 0:
                print(f"âŒ é”™è¯¯æ¬¡æ•°: {total_stats['errors_count']}")

            print(f"ðŸŽ‰ ä»»åŠ¡å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*80}")

            logger.info("Recommendation job completed", user_id=user_id, job_run_id=job_run_id, stats=total_stats)

            return {
                "status": "completed",
                "stats": total_stats,
                "job_run_id": job_run_id
            }

        except Exception as e:
            # Mark job as failed
            job_run.status = "failed"
            job_run.finished_at = func.now()
            job_run.error_message = str(e)
            job_run.counters = total_stats if 'total_stats' in locals() else {}
            self.db.commit()

            logger.error("Recommendation job failed", user_id=user_id, job_run_id=job_run_id, error=str(e))
            raise

    async def _fetch_repositories(
        self,
        github_client: GitHubClient,
        preference: Preference,
        force_refresh: bool = False
    ) -> List[Dict[str, Any]]:
        """Fetch repositories from GitHub based on preference"""

        # Check if we should use cached data
        if not force_refresh:
            # Look for recent cached data
            recent_repos = self.db.query(RepoCache).filter(
                RepoCache.fetched_at > datetime.utcnow() - preference.created_after if preference.created_after else datetime.utcnow() - timedelta(hours=1)
            ).limit(100).all()

            if recent_repos:
                logger.info("Using cached repository data", count=len(recent_repos))
                return [github_client.parse_repo_data(repo.github_data) for repo in recent_repos if repo.github_data]

        # Fetch from GitHub API
        repos = []

        # Handle large keyword lists by splitting them into multiple searches
        # GitHub API limits OR operators to 5, so we need to do multiple searches for better coverage
        max_keywords_per_search = 5
        keyword_chunks = []

        if preference.keywords:
            # Split keywords into chunks of 5
            for i in range(0, len(preference.keywords), max_keywords_per_search):
                chunk = preference.keywords[i:i + max_keywords_per_search]
                keyword_chunks.append(chunk)
        else:
            keyword_chunks = [[]]  # Empty search

        # Search for each language and keyword chunk combination
        if preference.languages:
            for language in preference.languages:
                for keyword_chunk in keyword_chunks:
                    try:
                        language_repos = await github_client.search_repositories(
                            keywords=keyword_chunk,
                            language=language,
                            min_stars=preference.min_stars,
                            created_after=preference.created_after,
                            updated_after=preference.updated_after,
                            per_page=30,  # Reduced to allow for more searches
                            max_pages=1   # Reduced to allow for more searches
                        )
                        repos.extend(language_repos)

                        # Log search results
                        logger.info("Search completed",
                                  language=language,
                                  keywords=keyword_chunk,
                                  results=len(language_repos))
                    except Exception as e:
                        logger.error("Search failed",
                                   language=language,
                                   keywords=keyword_chunk,
                                   error=str(e))
                        continue
        else:
            # Search without language filter using keyword chunks
            for keyword_chunk in keyword_chunks:
                try:
                    chunk_repos = await github_client.search_repositories(
                        keywords=keyword_chunk,
                        min_stars=preference.min_stars,
                        created_after=preference.created_after,
                        updated_after=preference.updated_after,
                        per_page=50,
                        max_pages=2
                    )
                    repos.extend(chunk_repos)

                    # Log search results
                    logger.info("Search completed",
                              keywords=keyword_chunk,
                              results=len(chunk_repos))
                except Exception as e:
                    logger.error("Search failed",
                               keywords=keyword_chunk,
                               error=str(e))
                    continue

        # Remove duplicates
        seen_ids = set()
        unique_repos = []
        for repo in repos:
            if repo["id"] not in seen_ids:
                seen_ids.add(repo["id"])
                unique_repos.append(repo)

        logger.info("Fetched repositories from GitHub", count=len(unique_repos))
        return unique_repos

    async def _cache_repositories(self, repos: List[Dict[str, Any]]) -> List[RepoCache]:
        """Cache repositories in the database"""
        cached_repos = []

        for repo_data in repos:
            try:
                # Parse repository data
                parsed_data = GitHubClient().parse_repo_data(repo_data)

                # Check if repo already exists in cache
                existing_repo = self.db.query(RepoCache).filter(
                    RepoCache.repo_id == parsed_data["repo_id"]
                ).first()

                if existing_repo:
                    # Update existing cache entry
                    for key, value in parsed_data.items():
                        if key != "repo_id":  # Don't update the primary key
                            setattr(existing_repo, key, value)
                    existing_repo.fetched_at = func.now()
                    cached_repos.append(existing_repo)
                else:
                    # Create new cache entry
                    repo_cache = RepoCache(**parsed_data)
                    self.db.add(repo_cache)
                    cached_repos.append(repo_cache)

            except Exception as e:
                logger.error("Failed to cache repository", repo_id=repo_data.get("id"), error=str(e))
                continue

        self.db.commit()
        return cached_repos

    def _repo_cache_to_dict(self, repo_cache: RepoCache) -> Dict[str, Any]:
        """Convert RepoCache model to dictionary for processing"""
        return {
            "repo_id": repo_cache.repo_id,
            "full_name": repo_cache.full_name,
            "name": repo_cache.name,
            "owner_login": repo_cache.owner_login,
            "description": repo_cache.description,
            "topics": repo_cache.topics,
            "language": repo_cache.language,
            "license_name": repo_cache.license_name,
            "stargazers_count": repo_cache.stargazers_count,
            "forks_count": repo_cache.forks_count,
            "watchers_count": repo_cache.watchers_count,
            "open_issues_count": repo_cache.open_issues_count,
            "size": repo_cache.size,
            "html_url": repo_cache.html_url,
            "clone_url": repo_cache.clone_url,
            "homepage": repo_cache.homepage,
            "is_private": repo_cache.is_private,
            "is_fork": repo_cache.is_fork,
            "is_archived": repo_cache.is_archived,
            "is_disabled": repo_cache.is_disabled,
            "created_at": repo_cache.created_at,
            "updated_at": repo_cache.updated_at,
            "pushed_at": repo_cache.pushed_at,
            "fetched_at": repo_cache.fetched_at
        }

    async def cleanup_old_data(self, days_old: int = 7) -> Dict[str, int]:
        """Clean up old cached data and job runs"""
        cutoff_date = datetime.utcnow() - timedelta(days=days_old)

        # Clean old repo cache entries
        old_repos = self.db.query(RepoCache).filter(
            RepoCache.fetched_at < cutoff_date
        ).count()

        self.db.query(RepoCache).filter(
            RepoCache.fetched_at < cutoff_date
        ).delete()

        # Clean old job runs
        old_jobs = self.db.query(JobRun).filter(
            JobRun.started_at < cutoff_date,
            JobRun.status.in_(["completed", "failed"])
        ).count()

        self.db.query(JobRun).filter(
            JobRun.started_at < cutoff_date,
            JobRun.status.in_(["completed", "failed"])
        ).delete()

        self.db.commit()

        return {
            "repos_cleaned": old_repos,
            "jobs_cleaned": old_jobs
        }